# 蒙特卡洛算法

## 不太短的简介

蒙特卡罗方法（英语：Monte Carlo Method），也称统计模拟方法，是1940年代中期由于科学技术的发展和电子计算机的发明，而提出的一种以概率统计理论为指导的数值计算方法。是指使用随机数（或更常见的伪随机数）来解决很多计算问题的方法。20世纪40年代，在科学家冯·诺伊曼、斯塔尼斯拉夫·乌拉姆和尼古拉斯·梅特罗波利斯于洛斯阿拉莫斯国家实验室为核武器计划工作时，发明了蒙特卡罗方法。因为乌拉姆的叔叔经常在摩纳哥的蒙特卡洛赌场输钱得名，而蒙特卡罗方法正是以概率为基础的方法。与它对应的是确定性算法。蒙特卡罗方法在金融工程学、宏观经济学、生物医学、计算物理学（如粒子输运计算、量子热力学计算、空气动力学计算）、机器学习等领域应用广泛

## 实际的应用

通常蒙特卡罗方法可以粗略地分成两类：一类是所求解的问题本身具有内在的随机性，借助计算机的运算能力可以直接模拟这种随机的过程。例如在核物理研究中，分析中子在反应堆中的传输过程。中子与原子核作用受到量子力学规律的制约，人们只能知道它们相互作用发生的概率，却无法准确获得中子与原子核作用时的位置以及裂变产生的新中子的行进速率和方向。科学家依据其概率进行随机抽样得到裂变位置、速度和方向，这样模拟大量中子的行为后，经过统计就能获得中子传输的范围，作为反应堆设计的依据。

另一种类型是所求解问题可以转化为某种随机分布的特征数，比如随机事件出现的概率，或者随机变量的期望值。通过随机抽样的方法，以随机事件出现的频率估计其概率，或者以抽样的数字特征估算随机变量的数字特征，并将其作为问题的解。这种方法多用于求解复杂的多维积分问题。

假设我们要计算一个不规则图形的面积，那么图形的不规则程度和分析性计算（比如，积分）的复杂程度是成正比的。蒙特卡罗方法基于这样的想法：假设你有一袋豆子，把豆子均匀地朝这个图形上撒，然后数这个图形之中有多少颗豆子，这个豆子的数目就是图形的面积。当你的豆子越小，撒的越多的时候，结果就越精确。借助计算机程序可以生成大量均匀分布坐标点，然后统计出图形内的点数，通过它们占总点数的比例和坐标点生成范围的面积就可以求出图形面积。

在解决实际问题的时候应用蒙特卡罗方法主要有两部分工作：

- 用蒙特卡罗方法模拟某一过程时，需要产生各种概率分布的随机变量。
- 用统计方法把模型的数字特征估计出来，从而得到实际问题的数值解。

## 在数学中的应用

使用蒙特卡罗方法估算π值：

<div align=center><img width=50% height=50% src=Pi的计算.png/></div>

蒙特卡罗方法可用于近似计算圆周率：让计算机每次随机生成两个0到2之间的数，看以这两个实数为横纵坐标的点是否在$x^2+y^2=4(x \ge 0，y\ge 0)$的$\frac{1}{4}$圆内。生成一系列随机点，统计单位圆内的点数与总点数，(1/4圆面积和正方形面积之比为$\pi$:1，$\pi$为圆周率)，当随机点获取越多时，其结果越接近于圆周率(然而准确度仍有争议：即使取10的9次方个随机点时，其结果也仅在前4位与圆周率吻合，此争议来源不清楚)。

用蒙特卡罗方法近似计算圆周率的先天不足是：第一，计算机产生的随机数是受到存储格式的限制的，是离散的，并不能产生连续的任意实数；上述做法将平面分割成一个个网格，在空间也不是连续的，由此计算出来的面积当然与圆或多或少有差距。

以下是使用蒙特卡洛算法计算$\pi$的代码实现：

```python
import os
import numpy as np
import matplotlib.pyplot as plt

os.system('cls')

x = np.linspace(0, 2, 1000)
y = np.sqrt(4 - x*x)

N = 3000
point_x = np.random.rand(N)*(2 - 0)
point_y = np.random.rand(N)*(2 - 0)

point_square_sum = point_x*point_x + point_y*point_y
const_distance = 4.0

point_x_red = point_x[point_square_sum <= const_distance]
point_y_red = point_y[point_square_sum <= const_distance]
point_x_blue = point_x[point_square_sum > const_distance]
point_y_blue = point_y[point_square_sum > const_distance]

N_red = len(point_x_red)
Pro = N_red/N
my_pi = 4.0*Pro

fig, ax = plt.subplots()

line, = ax.plot(x, y)

plt.plot(point_x_red, point_y_red, '.', color='red')
plt.plot(point_x_blue, point_y_blue, '.', color='blue')


plt.axis('scaled')
plt.title('n={} $\pi\\approx${}'.format(N, my_pi))
plt.savefig('Pi的计算.png',bbox_inches='tight',dpi=fig.dpi,pad_inches=0.0)
plt.show()
```

使用蒙特卡洛算法估算积分：
非权重蒙特卡罗积分，也称确定性抽样，是对被积函数变量区间进行随机均匀抽样，然后对抽样点的函数值求平均，从而可以得到函数积分的近似值。此种方法的正确性是基于概率论的中心极限定理。当抽样点数为m时，使用此种方法所得近似解的统计误差只与m有关（与$\frac{1}{\sqrt{m}}$正相关），不随积分维数的改变而改变。因此当积分维度较高时，蒙特卡罗方法相对于其他数值解法更优

蒙特卡洛积分表达为
$$
  F_{N} = \frac{b-a}{N} \sum_{i=1}^{N}f(X_{i})
$$
其中，$b-a$是宽，其余部分是高的平均。当N趋于无穷时，根据大数定律，样本的平均值，会无限趋近于期望值。即矩形的高无限趋近于其期望。但期望，等于真实值吗，下面可以证明：
$$
\begin{align*}
E[F_{N}] &= E[\frac{b-a}{N} \sum_{i=1}^{N}f(X_{i})] \\
         &= \frac{b-a}{N} \sum_{i=1}^{N}E[f(X_{i})] \\
         &= \frac{b-a}{N} \sum_{i=1}^{N}\int_{a}^{b}f(x)p(x)dx \\
         &= \frac{b-a}{N} \frac{1}{b-a} \sum_{i=1}^{N}\int_{a}^{b}f(x)dx \\
         &= \frac{1}{N} \sum_{i=1}^{N}\int_{a}^{b}f(x)dx \\
         &= \int_{a}^{b}f(x)dx \\
\end{align*}  
$$
估计量的数学期望等于被估计参数的真实值，我们的估计是无偏的。在均匀采样的情况下，蒙特卡洛积分公式为
$$
  \int_{a}^{b}f(x)dx = (b-a)\frac{1}{N}\sum_{i=1}^{N}f(X_{i})
$$

设被积函数为$lnx$，函数在积分区间为[1, 4]的积分为
$$
  I = \int_{1}^{4}lnxdx
$$
设在积分区间[1,4]上随机取样N个，而蒙特卡洛积分为
$$
  I = \int_{1}^{4}lnxdx = 3\frac{1}{N}\sum_{i=1}^{N}f(X_{i})
$$

```python
import numpy as np
import matplotlib.pyplot as plt

N = 3000000
point_x = np.random.rand(N)*4
point_x = point_x[point_x >= 1.0]

point_log = np.log(point_x)
res = np.average(point_log) * 3
print(res)
# res = 2.545614
```

## 在机器学习中的应用

蒙特卡洛算法也常用于机器学习，特别是强化学习的算法中。一般情况下，针对得到的样本数据集创建相对模糊的模型，通过蒙特卡洛方法对于模型中的参数进行选取，使之于原始数据的残差尽可能的小。从而达到创建模型拟合样本的目的
